{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from functools import reduce\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Fairlearn\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "from fairlearn.metrics import MetricFrame, selection_rate, equalized_odds_difference, true_positive_rate, false_positive_rate\n",
    "from fairlearn.metrics import *\n",
    "\n",
    "from matplotlib import rc\n",
    "rc('text', usetex=True)\n",
    "plt.rcParams['text.latex.preamble'] = r'''\n",
    "\\usepackage{mathtools}\n",
    "\n",
    "\\usepackage{helvet}\n",
    "\\renewcommand{\\familydefault}{\\sfdefault}\n",
    "% more packages here\n",
    "'''\n",
    "plt.rcParams['legend.title_fontsize'] = 14\n",
    "plt.rcParams[\"font.family\"] = 'sans-serif'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfrls_clients_df = pd.read_csv('../../data/raw/2021_8_26/LIM_REFERRAL_CLIENTS_08262021.csv')\n",
    "addendum_df = pd.read_csv('../../data/raw/2021_8_26/LIM_ADDENDUM_08262021.csv')\n",
    "maxscores_df = pd.read_csv('../../data/raw/2021_8_26/LIM_MAXSCORES_08262021.csv')\n",
    "allscores_df = pd.read_csv('../../data/raw/2021_8_26/LIM_ALLSCORES_08262021.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restrict dates\n",
    "df = rfrls_clients_df[(pd.to_datetime(rfrls_clients_df.INTAKE_DT) >= datetime.datetime(2015,1,1))\n",
    "            &(pd.to_datetime(rfrls_clients_df.INTAKE_DT) <= datetime.datetime(2018,7,31))]\n",
    "\n",
    "# exclude non-GPS referrals\n",
    "df = df[~rfrls_clients_df.REF_TYPE.isna()]\n",
    "df = df[df.REF_TYPE == 'GPS']\n",
    "\n",
    "# exclude active referrals (exclude ACTIVE_FAMILY_IND == 1)\n",
    "active_rfrl_ids = addendum_df[addendum_df.ACTIVE_FAMILY_IND == 1]['REFER_ID']\n",
    "df = df[~df.REFER_ID.isin(active_rfrl_ids)]\n",
    "\n",
    "# exclude truancy referrals (exclude TRUANCY_ONLY_COURTS_REF == 1)\n",
    "truancy_rfrl_ids = addendum_df[addendum_df.TRUANCY_ONLY_COURTS_REF == 1]['REFER_ID']\n",
    "df = df[~df.REFER_ID.isin(truancy_rfrl_ids)]\n",
    "\n",
    "# include only children\n",
    "df = df[df.ALL_CHILD == 1]\n",
    "\n",
    "# add CALL_SCRN_CODE (Screen-in if CALL_SCRN_CODE==1 else screen-out)\n",
    "df = df.join(addendum_df[['REFER_ID','CALL_SCRN_CODE']].set_index('REFER_ID'), on='REFER_ID')\n",
    "\n",
    "# exclude referrals intake didn't screen (CALL_SCRN_CODE == -9)\n",
    "df = df[df.CALL_SCRN_CODE != -9]\n",
    "\n",
    "# exclude referral-children without an MCI ID\n",
    "df = df[~df.MCI_ID.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add numbers of future removals and rereferrals within 2 yrs and 2 months, respectively\n",
    "rmvl_df = pd.read_csv('../../data/raw/2021_8_26/LIM_PLACEMENTS_08262021.csv')\n",
    "\n",
    "df.INTAKE_DT = pd.to_datetime(df.INTAKE_DT)\n",
    "rmvl_df.RMVL_ENTRY_DT = pd.to_datetime(rmvl_df.RMVL_ENTRY_DT)\n",
    "\n",
    "print(\"running...\")\n",
    "df['num_rmvl_730'] = df.apply(lambda x :\n",
    "                                    np.count_nonzero((rmvl_df.MCI_ID == x.MCI_ID)\n",
    "                                        &(x.INTAKE_DT <= rmvl_df.RMVL_ENTRY_DT)\n",
    "                                        &(x.INTAKE_DT > rmvl_df.RMVL_ENTRY_DT-datetime.timedelta(730))), axis=1)\n",
    "\n",
    "df['num_rerfrl_62'] = df.apply(lambda x :\n",
    "                                    np.count_nonzero((df.MCI_ID == x.MCI_ID)\n",
    "                                        &(df.INTAKE_DT > x.INTAKE_DT)\n",
    "                                        &(df.INTAKE_DT < x.INTAKE_DT + datetime.timedelta(62))), axis=1)\n",
    "\n",
    "df['HighRiskOutcome'] = ((df['num_rmvl_730'] > 0) | (df['num_rerfrl_62'] > 0)).astype(int)\n",
    "\n",
    "\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude referrals with RACE == NaN\n",
    "df = df[~df.RACE.isna()]\n",
    "\n",
    "# make new RACE_CYF var (same as RACE from original data)\n",
    "df.RACE_CYF = df.RACE.copy()\n",
    "\n",
    "# recode RACE var: \n",
    "# RACE = 'white' if RACE_CYF == 'White' \n",
    "# RACE = 'Black' if RACE_CYF contains 'Black or African American'\n",
    "# RACE = 'other' if RACE (new var) is neither 'white' nor 'Black'\n",
    "df.loc[df.RACE_CYF == 'White', 'RACE'] = 'white'\n",
    "df.loc[df.RACE_CYF.str.contains('Black or African American'), 'RACE'] = 'Black'\n",
    "df.loc[~df.RACE.isin(['white','Black']), 'RACE'] = 'other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data by dates\n",
    "\n",
    "# pre_df from 1/1/15 to 7/31/16\n",
    "pre_df = df[(pd.to_datetime(df.INTAKE_DT) >= datetime.datetime(2015,1,1))\n",
    "            &(pd.to_datetime(df.INTAKE_DT) <= datetime.datetime(2016,7,31))]\n",
    "\n",
    "# post_df from 12/1/16 to 5/31/18\n",
    "post_df = df[(pd.to_datetime(df.INTAKE_DT) >= datetime.datetime(2016,8,1))\n",
    "            &(pd.to_datetime(df.INTAKE_DT) <= datetime.datetime(2018,5,13))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(pre_df.MCI_ID.to_list()+post_df.MCI_ID.to_list()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-AFST\n",
    "print(\"pre all # entries\",len(pre_df))\n",
    "print(\"pre all # children\",len(pre_df.MCI_ID.unique()))\n",
    "print(\"pre all # referrals\",len(pre_df.REFER_ID.unique()))\n",
    "print(\"pre all # entries screen-in\",len(pre_df[pre_df.CALL_SCRN_CODE == 1]))\n",
    "print(\"pre all # entries screen-out\",len(pre_df[pre_df.CALL_SCRN_CODE == 0]))\n",
    "print(\"pre all screen-in %\",len(pre_df[pre_df.CALL_SCRN_CODE == 1])/len(pre_df))\n",
    "\n",
    "print()\n",
    "print(\"pre Black # entries\",len(pre_df[pre_df.RACE=='Black']))\n",
    "print(\"pre Black % of rfrls\",len(pre_df[pre_df.RACE=='Black'])/len(pre_df))\n",
    "print(\"pre Black # screen-in\",len(pre_df[(pre_df.RACE=='Black')&(pre_df.CALL_SCRN_CODE == 1)]))\n",
    "print(\"pre Black # screen-out\",len(pre_df[(pre_df.RACE=='Black')&(pre_df.CALL_SCRN_CODE == 0)]))\n",
    "print(\"pre Black screen-in %\",len(pre_df[(pre_df.RACE=='Black')&(pre_df.CALL_SCRN_CODE == 1)])/len(pre_df[pre_df.RACE=='Black']))\n",
    "\n",
    "print()\n",
    "print(\"pre white # entries\",len(pre_df[pre_df.RACE=='white']))\n",
    "print(\"pre white % of rfrls\",len(pre_df[pre_df.RACE=='white'])/len(pre_df))\n",
    "print(\"pre white # screen-in\",len(pre_df[(pre_df.RACE=='white')&(pre_df.CALL_SCRN_CODE == 1)]))\n",
    "print(\"pre white # screen-out\",len(pre_df[(pre_df.RACE=='white')&(pre_df.CALL_SCRN_CODE == 0)]))\n",
    "print(\"pre white screen-in %\",len(pre_df[(pre_df.RACE=='white')&(pre_df.CALL_SCRN_CODE == 1)])/len(pre_df[pre_df.RACE=='white']))\n",
    "\n",
    "# post-AFST\n",
    "print()\n",
    "print(\"post all # entries\",len(post_df))\n",
    "print(\"post all # children\",len(post_df.MCI_ID.unique()))\n",
    "print(\"post all # referrals\",len(post_df.REFER_ID.unique()))\n",
    "print(\"post all # screen-in\",len(post_df[post_df.CALL_SCRN_CODE == 1]))\n",
    "print(\"post all # screen-out\",len(post_df[post_df.CALL_SCRN_CODE == 0]))\n",
    "print(\"post all screen-in %\",len(post_df[post_df.CALL_SCRN_CODE == 1])/len(post_df))\n",
    "\n",
    "print()\n",
    "print(\"post Black # entries\",len(post_df[post_df.RACE=='Black']))\n",
    "print(\"post Black % of rfrls\",len(post_df[post_df.RACE=='Black'])/len(post_df))\n",
    "print(\"post Black # screen-in\",len(post_df[(post_df.RACE=='Black')&(post_df.CALL_SCRN_CODE == 1)]))\n",
    "print(\"post Black # screen-out\",len(post_df[(post_df.RACE=='Black')&(post_df.CALL_SCRN_CODE == 0)]))\n",
    "print(\"post Black screen-in %\",len(post_df[(post_df.RACE=='Black')&(post_df.CALL_SCRN_CODE == 1)])/len(post_df[post_df.RACE=='Black']))\n",
    "\n",
    "print()\n",
    "print(\"post white # entries\",len(post_df[post_df.RACE=='white']))\n",
    "print(\"post white % of rfrls\",len(post_df[post_df.RACE=='white'])/len(post_df))\n",
    "print(\"post white # screen-in\",len(post_df[(post_df.RACE=='white')&(post_df.CALL_SCRN_CODE == 1)]))\n",
    "print(\"post white # screen-out\",len(post_df[(post_df.RACE=='white')&(post_df.CALL_SCRN_CODE == 0)]))\n",
    "print(\"post white screen-in %\",len(post_df[(post_df.RACE=='white')&(post_df.CALL_SCRN_CODE == 1)])/len(post_df[post_df.RACE=='white']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add scores\n",
    "PAN_Retro data contains AFST scores run retroactively, so they do not contain the glitch that caused erroneous AFST scores to be shown to call screen workers throughout Version 1 of the AFST. About 97% of all Black and white children in the LIM_REFERRAL_CLIENTS data were matched with a score in the PAN_Retro data (as shown below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfrl_1416 = pd.read_csv('../../data/stanford_data/PAN_Retro_Run_Referrals_for_2014-2016_provided_2018-08-29.csv', skiprows=1)\n",
    "rfrl_1618 = pd.read_csv('../../data/stanford_data/PAN_Retro_Run_08012016_07132018.csv')\n",
    "\n",
    "# Convert raw AFST scores to 1-20, add max scores, and add risk levels\n",
    "RR_score_df = pd.read_csv('../../data/stanford_data/RR_scorecutoffs.csv')\n",
    "PL_score_df = pd.read_csv('../../data/stanford_data/PL_scorecutoffs.csv')\n",
    "\n",
    "RR_bin = RR_score_df['MIN_RISK'].to_list()+[999999.0]\n",
    "RR_score = RR_score_df['SCORE'].to_list()\n",
    "PL_bin = PL_score_df['MIN_RISK'].to_list()+[999999.0]\n",
    "PL_score = PL_score_df['SCORE'].to_list()\n",
    "\n",
    "rfrl_1416['RR_converted'] = pd.cut(rfrl_1416['RR_SCORE'], bins=RR_bin, labels=RR_score)\n",
    "rfrl_1416['PL_converted'] = pd.cut(rfrl_1416['PL_SCORE'], bins=PL_bin, labels=PL_score)\n",
    "rfrl_1618['RR_converted'] = pd.cut(rfrl_1618['RR_SCORE'], bins=RR_bin, labels=RR_score)\n",
    "rfrl_1618['PL_converted'] = pd.cut(rfrl_1618['PL_SCORE'], bins=PL_bin, labels=PL_score)\n",
    "\n",
    "# convert to float so they match MAX_score\n",
    "rfrl_1416['RR_converted'] = rfrl_1416['RR_converted'].astype(float)\n",
    "rfrl_1416['PL_converted'] = rfrl_1416['PL_converted'].astype(float)\n",
    "rfrl_1618['RR_converted'] = rfrl_1618['RR_converted'].astype(float)\n",
    "rfrl_1618['PL_converted'] = rfrl_1618['PL_converted'].astype(float)\n",
    "\n",
    "# add MAX_score (max of either RR_converted or PL_converted)\n",
    "rfrl_1416['MAX_score'] = rfrl_1416[['PL_converted','RR_converted']].max(axis=1)\n",
    "rfrl_1618['MAX_score'] = rfrl_1618[['PL_converted','RR_converted']].max(axis=1)\n",
    "\n",
    "# pick out max PL or RR score out of all children in referral\n",
    "rfrl_1416_score = rfrl_1416.groupby(['REFER_ID'])['MAX_score'].max().reset_index()\n",
    "rfrl_1618_score = rfrl_1618.groupby(['REFER_ID'])['MAX_score'].max().reset_index()\n",
    "\n",
    "# save the old (per-mci_id) Max_score\n",
    "rfrl_1416['MAX_score_individual'] = rfrl_1416['MAX_score']\n",
    "rfrl_1618['MAX_score_individual'] = rfrl_1618['MAX_score']\n",
    "\n",
    "# drop the old (per-mci_id) Max_score\n",
    "rfrl_1416 = rfrl_1416.drop(['MAX_score'], axis=1)\n",
    "rfrl_1618 = rfrl_1618.drop(['MAX_score'], axis=1)\n",
    "\n",
    "# Join with Max score per referral\n",
    "rfrl_1416 = pd.merge(how=\"inner\", left=rfrl_1416, right=rfrl_1416_score[['REFER_ID', 'MAX_score']], on=\"REFER_ID\")\n",
    "rfrl_1618 = pd.merge(how=\"inner\", left=rfrl_1618, right=rfrl_1618_score[['REFER_ID', 'MAX_score']], on=\"REFER_ID\")\n",
    "\n",
    "# convert RR & PL risk scores to risk levels (Low, Med, High)\n",
    "level_bin = [0,9,14,20]\n",
    "level_labels = ['Low_risk', 'Medium_risk', 'High_risk']\n",
    "\n",
    "rfrl_1416['RR_Risk_level'] = pd.cut(rfrl_1416['RR_converted'], bins=level_bin, labels=level_labels)\n",
    "rfrl_1416['PL_Risk_level'] = pd.cut(rfrl_1416['PL_converted'], bins=level_bin, labels=level_labels)\n",
    "rfrl_1416['MAX_level'] = pd.cut(rfrl_1416['MAX_score'], bins=level_bin, labels=level_labels)\n",
    "\n",
    "rfrl_1618['RR_Risk_level'] = pd.cut(rfrl_1618['RR_converted'], bins=level_bin, labels=level_labels)\n",
    "rfrl_1618['PL_Risk_level'] = pd.cut(rfrl_1618['PL_converted'], bins=level_bin, labels=level_labels)\n",
    "rfrl_1618['MAX_level'] = pd.cut(rfrl_1618['MAX_score'], bins=level_bin, labels=level_labels)\n",
    "\n",
    "# add mandatory screening column\n",
    "rfrl_1416['MANDATORY_NULL'] = (rfrl_1416['PL_converted'] >= 18).astype(int)\n",
    "rfrl_1618['MANDATORY_NULL'] = (rfrl_1618['PL_converted'] >= 18).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_df = pre_df.join(rfrl_1416[['REFER_ID','MCI_ID','PL_converted','MAX_score','MANDATORY_NULL']].set_index(['REFER_ID','MCI_ID']), on = ['REFER_ID','MCI_ID'], how='left')\n",
    "post_df = post_df.join(rfrl_1618[['REFER_ID','MCI_ID','PL_converted','MAX_score','MANDATORY_NULL']].set_index(['REFER_ID','MCI_ID']), on = ['REFER_ID','MCI_ID'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure MAX_score is consistent across all REFER_ID in pre_df and post_df\n",
    "# pick out max PL or RR score out of all children in referral\n",
    "pre_df_score = pre_df.groupby(['REFER_ID'])['MAX_score'].max().reset_index()\n",
    "post_df_score = post_df.groupby(['REFER_ID'])['MAX_score'].max().reset_index()\n",
    "\n",
    "# save the old (per-mci_id) Max_score\n",
    "pre_df['MAX_score_individual'] = pre_df['MAX_score']\n",
    "post_df['MAX_score_individual'] = post_df['MAX_score']\n",
    "\n",
    "# drop the old (per-mci_id) Max_score\n",
    "pre_df = pre_df.drop(['MAX_score'], axis=1)\n",
    "post_df = post_df.drop(['MAX_score'], axis=1)\n",
    "\n",
    "# Join with Max score per referral\n",
    "pre_df = pd.merge(how=\"inner\", left=pre_df, right=pre_df_score[['REFER_ID', 'MAX_score']], on=\"REFER_ID\")\n",
    "post_df = pd.merge(how=\"inner\", left=post_df, right=post_df_score[['REFER_ID', 'MAX_score']], on=\"REFER_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add MAX_level\n",
    "def max_level_bins(score):\n",
    "    if score < 10: return 'Low_risk'\n",
    "    elif score < 15: return 'Medium_risk'\n",
    "    else: return 'High_risk'\n",
    "\n",
    "pre_df['MAX_level'] = pre_df.MAX_score.apply(lambda x : max_level_bins(x))\n",
    "post_df['MAX_level'] = post_df.MAX_score.apply(lambda x : max_level_bins(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure MANDATORY_NULL is consistent across all REFER_ID in pre_df and post_df\n",
    "# pick out max MANDATORY_NULL of all children in referral\n",
    "pre_df_score = pre_df.groupby(['REFER_ID'])['MANDATORY_NULL'].max().reset_index()\n",
    "post_df_score = post_df.groupby(['REFER_ID'])['MANDATORY_NULL'].max().reset_index()\n",
    "\n",
    "# drop the old (per-mci_id) MANDATORY_NULL\n",
    "pre_df = pre_df.drop(['MANDATORY_NULL'], axis=1)\n",
    "post_df = post_df.drop(['MANDATORY_NULL'], axis=1)\n",
    "\n",
    "# Join with MANDATORY_NULL per referral\n",
    "pre_df = pd.merge(how=\"inner\", left=pre_df, right=pre_df_score[['REFER_ID', 'MANDATORY_NULL']], on=\"REFER_ID\")\n",
    "post_df = pd.merge(how=\"inner\", left=post_df, right=post_df_score[['REFER_ID', 'MANDATORY_NULL']], on=\"REFER_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('% no scores in PAN_Retro (pre):',np.count_nonzero(pre_df.MAX_score.isna())/len(pre_df))\n",
    "print('% no scores in PAN_Retro (post):',np.count_nonzero(post_df.MAX_score.isna())/len(post_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('% no scores in PAN_Retro (post white):',\n",
    "      np.count_nonzero((post_df.MAX_score.isna())&(post_df.RACE=='white'))/len(post_df[post_df.RACE=='white']))\n",
    "\n",
    "print('% no scores in PAN_Retro (post Black):',\n",
    "      np.count_nonzero((post_df.MAX_score.isna())&(post_df.RACE=='Black'))/len(post_df[post_df.RACE=='Black']))\n",
    "\n",
    "print('% no scores in PAN_Retro (pre white):',\n",
    "      np.count_nonzero((pre_df.MAX_score.isna())&(pre_df.RACE=='white'))/len(pre_df[pre_df.RACE=='white']))\n",
    "\n",
    "print('% no scores in PAN_Retro (pre Black):',\n",
    "      np.count_nonzero((pre_df.MAX_score.isna())&(pre_df.RACE=='Black'))/len(pre_df[pre_df.RACE=='Black']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pct of referrals with no score in PAN_Retro have MCI_ID in PAN_Retro\n",
    "len(post_df[(post_df.MAX_score.isna())\n",
    "          &(post_df.MCI_ID.isin(rfrl_1618.MCI_ID))])/len(post_df[(post_df.MAX_score.isna())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_original = post_df.copy()\n",
    "pre_original = pre_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(post_df[(post_df.MAX_score.isna())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(post_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude referrals with no retroactive score\n",
    "post_df = post_df[~post_df.MAX_score.isna()]\n",
    "pre_df = pre_df[~pre_df.MAX_score.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to file\n",
    "pre_df.to_csv('../../data/processed/disparity_paper/pre_AFST_with_race_outcomes.csv',index=False)\n",
    "post_df.to_csv('../../data/processed/disparity_paper/post_AFST_with_race_outcomes.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-AFST\n",
    "print(\"AFST-only screening rates, i.e. AFST score >= 15 or not\")\n",
    "print(\"pre all # entries\",len(pre_df))\n",
    "print(\"pre all # screen-in\",len(pre_df[pre_df.MAX_score >= 15]))\n",
    "print(\"pre all # screen-out\",len(pre_df[pre_df.MAX_score < 15]))\n",
    "print(\"pre all screen-in %\",len(pre_df[pre_df.MAX_score >= 15])/len(pre_df))\n",
    "\n",
    "print()\n",
    "print(\"pre Black # entries\",len(pre_df[pre_df.RACE=='Black']))\n",
    "print(\"pre Black % of rfrls\",len(pre_df[pre_df.RACE=='Black'])/len(pre_df))\n",
    "print(\"pre Black # screen-in\",len(pre_df[(pre_df.RACE=='Black')&(pre_df.MAX_score >= 15)]))\n",
    "print(\"pre Black # screen-out\",len(pre_df[(pre_df.RACE=='Black')&(pre_df.MAX_score < 15)]))\n",
    "print(\"pre Black screen-in %\",len(pre_df[(pre_df.RACE=='Black')&(pre_df.MAX_score >= 15)])/len(pre_df[pre_df.RACE=='Black']))\n",
    "\n",
    "print()\n",
    "print(\"pre white # entries\",len(pre_df[pre_df.RACE=='white']))\n",
    "print(\"pre white % of rfrls\",len(pre_df[pre_df.RACE=='white'])/len(pre_df))\n",
    "print(\"pre white # screen-in\",len(pre_df[(pre_df.RACE=='white')&(pre_df.MAX_score >= 15)]))\n",
    "print(\"pre white # screen-out\",len(pre_df[(pre_df.RACE=='white')&(pre_df.MAX_score < 15)]))\n",
    "print(\"pre white screen-in %\",len(pre_df[(pre_df.RACE=='white')&(pre_df.MAX_score >= 15)])/len(pre_df[pre_df.RACE=='white']))\n",
    "\n",
    "# post-AFST\n",
    "print()\n",
    "print(\"post all # entries\",len(post_df))\n",
    "print(\"post all # screen-in\",len(post_df[post_df.MAX_score >= 15]))\n",
    "print(\"post all # screen-out\",len(post_df[post_df.MAX_score < 15]))\n",
    "print(\"post all screen-in %\",len(post_df[post_df.MAX_score >= 15])/len(post_df))\n",
    "\n",
    "print()\n",
    "print(\"post Black # entries\",len(post_df[post_df.RACE=='Black']))\n",
    "print(\"post Black % of rfrls\",len(post_df[post_df.RACE=='Black'])/len(post_df))\n",
    "print(\"post Black # screen-in\",len(post_df[(post_df.RACE=='Black')&(post_df.MAX_score >= 15)]))\n",
    "print(\"post Black # screen-out\",len(post_df[(post_df.RACE=='Black')&(post_df.MAX_score < 15)]))\n",
    "print(\"post Black screen-in %\",len(post_df[(post_df.RACE=='Black')&(post_df.MAX_score >= 15)])/len(post_df[post_df.RACE=='Black']))\n",
    "\n",
    "print()\n",
    "print(\"post white # entries\",len(post_df[post_df.RACE=='white']))\n",
    "print(\"post white % of rfrls\",len(post_df[post_df.RACE=='white'])/len(post_df))\n",
    "print(\"post white # screen-in\",len(post_df[(post_df.RACE=='white')&(post_df.MAX_score >= 15)]))\n",
    "print(\"post white # screen-out\",len(post_df[(post_df.RACE=='white')&(post_df.MAX_score < 15)]))\n",
    "print(\"post white screen-in %\",len(post_df[(post_df.RACE=='white')&(post_df.MAX_score >= 15)])/len(post_df[post_df.RACE=='white']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-AFST\n",
    "print(\"AFST-only screening rates for mandatory screen-in\")\n",
    "print(\"pre all # entries\",len(pre_df))\n",
    "print(\"pre all # screen-in\",len(pre_df[pre_df.MANDATORY_NULL == 1]))\n",
    "print(\"pre all # screen-out\",len(pre_df[pre_df.MANDATORY_NULL == 0]))\n",
    "print(\"pre all screen-in %\",len(pre_df[pre_df.MANDATORY_NULL == 1])/len(pre_df))\n",
    "\n",
    "print()\n",
    "print(\"pre Black # entries\",len(pre_df[pre_df.RACE=='Black']))\n",
    "print(\"pre Black % of rfrls\",len(pre_df[pre_df.RACE=='Black'])/len(pre_df))\n",
    "print(\"pre Black # screen-in\",len(pre_df[(pre_df.RACE=='Black')&(pre_df.MANDATORY_NULL == 1)]))\n",
    "print(\"pre Black # screen-out\",len(pre_df[(pre_df.RACE=='Black')&(pre_df.MANDATORY_NULL == 0)]))\n",
    "print(\"pre Black screen-in %\",len(pre_df[(pre_df.RACE=='Black')&(pre_df.MANDATORY_NULL == 1)])/len(pre_df[pre_df.RACE=='Black']))\n",
    "\n",
    "print()\n",
    "print(\"pre white # entries\",len(pre_df[pre_df.RACE=='white']))\n",
    "print(\"pre white % of rfrls\",len(pre_df[pre_df.RACE=='white'])/len(pre_df))\n",
    "print(\"pre white # screen-in\",len(pre_df[(pre_df.RACE=='white')&(pre_df.MANDATORY_NULL == 1)]))\n",
    "print(\"pre white # screen-out\",len(pre_df[(pre_df.RACE=='white')&(pre_df.MANDATORY_NULL == 0)]))\n",
    "print(\"pre white screen-in %\",len(pre_df[(pre_df.RACE=='white')&(pre_df.MANDATORY_NULL == 1)])/len(pre_df[pre_df.RACE=='white']))\n",
    "\n",
    "# post-AFST\n",
    "print()\n",
    "print(\"post all # entries\",len(post_df))\n",
    "print(\"post all # screen-in\",len(post_df[post_df.MANDATORY_NULL == 1]))\n",
    "print(\"post all # screen-out\",len(post_df[post_df.MANDATORY_NULL == 0]))\n",
    "print(\"post all screen-in %\",len(post_df[post_df.MANDATORY_NULL == 1])/len(post_df))\n",
    "\n",
    "print()\n",
    "print(\"post Black # entries\",len(post_df[post_df.RACE=='Black']))\n",
    "print(\"post Black % of rfrls\",len(post_df[post_df.RACE=='Black'])/len(post_df))\n",
    "print(\"post Black # screen-in\",len(post_df[(post_df.RACE=='Black')&(post_df.MANDATORY_NULL == 1)]))\n",
    "print(\"post Black # screen-out\",len(post_df[(post_df.RACE=='Black')&(post_df.MANDATORY_NULL == 0)]))\n",
    "print(\"post Black screen-in %\",len(post_df[(post_df.RACE=='Black')&(post_df.MANDATORY_NULL == 1)])/len(post_df[post_df.RACE=='Black']))\n",
    "\n",
    "print()\n",
    "print(\"post white # entries\",len(post_df[post_df.RACE=='white']))\n",
    "print(\"post white % of rfrls\",len(post_df[post_df.RACE=='white'])/len(post_df))\n",
    "print(\"post white # screen-in\",len(post_df[(post_df.RACE=='white')&(post_df.MANDATORY_NULL == 1)]))\n",
    "print(\"post white # screen-out\",len(post_df[(post_df.RACE=='white')&(post_df.MANDATORY_NULL == 0)]))\n",
    "print(\"post white screen-in %\",len(post_df[(post_df.RACE=='white')&(post_df.MANDATORY_NULL == 1)])/len(post_df[post_df.RACE=='white']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D\n",
    "\n",
    "pre_black = pre_df.loc[pre_df.RACE=='Black']\n",
    "pre_white = pre_df.loc[pre_df.RACE=='white']\n",
    "post_black = post_df.loc[post_df.RACE=='Black']\n",
    "post_white = post_df.loc[post_df.RACE=='white']\n",
    "\n",
    "afstonly_screenrate_black = 100*np.count_nonzero(post_black['MAX_score']>=15)/len(post_black)\n",
    "afstonly_screenrate_white = 100*np.count_nonzero(post_white['MAX_score']>=15)/len(post_white)\n",
    "workerafst_screenrate_black = 100*np.count_nonzero(post_black['CALL_SCRN_CODE']==1)/len(post_black)\n",
    "workerafst_screenrate_white = 100*np.count_nonzero(post_white['CALL_SCRN_CODE']==1)/len(post_white)\n",
    "\n",
    "afstonly_std_black = np.sqrt((100*(post_black['MAX_score']>=15)).var()/len(post_black))\n",
    "afstonly_std_white = np.sqrt((100*(post_white['MAX_score']>=15)).var()/len(post_white))\n",
    "workerafst_std_black = np.sqrt((100*(post_black['CALL_SCRN_CODE']==1)).var()/len(post_black))\n",
    "workerafst_std_white = np.sqrt((100*(post_white['CALL_SCRN_CODE']==1)).var()/len(post_white))\n",
    "\n",
    "std_black = [0,afstonly_std_black,workerafst_std_black,0]\n",
    "std_white = [afstonly_std_white,workerafst_std_white]\n",
    "\n",
    "data_black = {'':-10, 'AFST-only\\n(2016-18)': afstonly_screenrate_black, 'worker-AFST\\n(2016-18)': workerafst_screenrate_black, ' ':-10}\n",
    "data_white = {'AFST-only\\n(2016-18)': afstonly_screenrate_white, 'worker-AFST\\n(2016-18)': workerafst_screenrate_white}\n",
    "\n",
    "legend_elements = [Line2D([0], [0], marker='o', color='w', label='Black children', markerfacecolor='tab:blue', markersize=7),\n",
    "                   Line2D([0], [0], marker='o', color='w', label='white children', markerfacecolor='tab:orange', markersize=7)]\n",
    "\n",
    "line_afstonly = [[1, afstonly_screenrate_black],[1, afstonly_screenrate_white]]\n",
    "line_workerafst = [[2, workerafst_screenrate_black],[2, workerafst_screenrate_white]]\n",
    "\n",
    "plt.figure(figsize=(5, 4), dpi=160)\n",
    "plt.plot([line_afstonly[0][0], line_afstonly[1][0]], [line_afstonly[0][1], line_afstonly[1][1]], color='r', linestyle=':',zorder=1)\n",
    "plt.plot([line_workerafst[0][0], line_workerafst[1][0]], [line_workerafst[0][1], line_workerafst[1][1]], color='r', linestyle=':',zorder=1)\n",
    "plt.scatter(data_black.keys(), data_black.values(),zorder=2)\n",
    "plt.scatter(data_white.keys(), data_white.values(),zorder=2)\n",
    "#plt.errorbar(data_black.keys(), data_black.values(), yerr=std_black, fmt='o', zorder=2)\n",
    "#plt.errorbar(data_white.keys(), data_white.values(), yerr=std_white, fmt='o', zorder=2)\n",
    "plt.text(1.1,(afstonly_screenrate_black+afstonly_screenrate_white)/2-4, str(round(afstonly_screenrate_black-afstonly_screenrate_white,1))+'\\% \\ndisparity',color='r')\n",
    "plt.text(2.1,(workerafst_screenrate_black+workerafst_screenrate_white)/2-5, str(round(workerafst_screenrate_black-workerafst_screenrate_white,1))+'\\% \\ndisparity',color='r',zorder=3)\n",
    "plt.ylim(0,100)\n",
    "axes = plt.gca()\n",
    "axes.yaxis.grid()\n",
    "plt.xlim(0.25,2.75)\n",
    "plt.title('Black-white screen-in rate disparities')\n",
    "plt.ylabel('Screen-in rate (\\%)',fontsize=14)\n",
    "plt.legend(handles=legend_elements)\n",
    "plt.savefig('../../figures/change_in_screen_in_rate.png', dpi=600, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worker-AFST vs AFST-only Screening Rate Disparities by threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "humanAI_black_screen_groupby = post_df.groupby(['RACE','CALL_SCRN_CODE']).size()\n",
    "humanAI_black_groupby = post_df.groupby(['RACE']).size()\n",
    "humanAI_screen_groupby = post_df.groupby('CALL_SCRN_CODE').size()\n",
    "\n",
    "num_black_screenin_humanAI = humanAI_black_screen_groupby['Black',1]\n",
    "num_black_rfrl_humanAI = humanAI_black_groupby['Black']\n",
    "num_white_screenin_humanAI = humanAI_black_screen_groupby['white',1]\n",
    "num_white_rfrl_humanAI = humanAI_black_groupby['white']\n",
    "num_screenin_humanAI = humanAI_screen_groupby[0]\n",
    "num_rfrl_humanAI = len(post_df)\n",
    "\n",
    "humanAI_black_screenin_rate = num_black_screenin_humanAI/num_black_rfrl_humanAI\n",
    "humanAI_white_screenin_rate = num_white_screenin_humanAI/num_white_rfrl_humanAI\n",
    "humanAI_disparity = humanAI_black_screenin_rate - humanAI_white_screenin_rate\n",
    "humanAI_overall_screenin_rate = num_screenin_humanAI/num_rfrl_humanAI\n",
    "\n",
    "humanAI_pre_black_screen_groupby = pre_df.groupby(['RACE','CALL_SCRN_CODE']).size()\n",
    "humanAI_pre_black_groupby = pre_df.groupby(['RACE']).size()\n",
    "humanAI_pre_screen_groupby = pre_df.groupby('CALL_SCRN_CODE').size()\n",
    "\n",
    "num_black_screenin_humanAI_pre = humanAI_pre_black_screen_groupby['Black',1]\n",
    "num_black_rfrl_humanAI_pre = humanAI_pre_black_groupby['Black']\n",
    "num_white_screenin_humanAI_pre = humanAI_pre_black_screen_groupby['white',1]\n",
    "num_white_rfrl_humanAI_pre = humanAI_pre_black_groupby['white']\n",
    "num_screenin_humanAI_pre = humanAI_pre_screen_groupby[1]\n",
    "num_rfrl_humanAI_pre = len(pre_df)\n",
    "\n",
    "humanAI_pre_black_screenin_rate = num_black_screenin_humanAI_pre/num_black_rfrl_humanAI_pre\n",
    "humanAI_pre_white_screenin_rate = num_white_screenin_humanAI_pre/num_white_rfrl_humanAI_pre\n",
    "humanAI_pre_disparity = humanAI_pre_black_screenin_rate - humanAI_pre_white_screenin_rate\n",
    "humanAI_pre_overall_screenin_rate = num_screenin_humanAI_pre/num_rfrl_humanAI_pre\n",
    "\n",
    "# define df for cases <= May 13, 2018\n",
    "post_df = post_df.loc[pd.to_datetime(post_df['INTAKE_DT']) <= pd.to_datetime('2018-05-13')]\n",
    "\n",
    "AI_overall_screenin_rate = {}\n",
    "AI_black_screenin_rate = {}\n",
    "AI_white_screenin_rate = {}\n",
    "AI_disparity_screenin_rate = {}\n",
    "\n",
    "for i in range(1,21):\n",
    "    AI_overall_screenin_rate[i] = np.count_nonzero(post_df['MAX_score'] >= i)/len(post_df)\n",
    "    AI_black_screenin_rate[i] = np.count_nonzero(post_df.loc[post_df['RACE']=='Black']['MAX_score'] >= i)/len(post_df.loc[post_df['RACE']=='Black'])\n",
    "    AI_white_screenin_rate[i] = np.count_nonzero(post_df.loc[post_df['RACE']=='white']['MAX_score'] >= i)/len(post_df.loc[post_df['RACE']=='white'])\n",
    "    AI_disparity_screenin_rate[i] = AI_white_screenin_rate[i]-AI_black_screenin_rate[i]\n",
    "    \n",
    "    \n",
    "AI_overall_screenin_rate['M'] = np.count_nonzero(post_df['MANDATORY_NULL'])/len(post_df)\n",
    "AI_black_screenin_rate['M'] = np.count_nonzero(post_df.loc[post_df['RACE']=='Black']['MANDATORY_NULL'])/len(post_df.loc[post_df['RACE']=='Black'])\n",
    "AI_white_screenin_rate['M'] = np.count_nonzero(post_df.loc[post_df['RACE']=='white']['MANDATORY_NULL'])/len(post_df.loc[post_df['RACE']=='white'])\n",
    "AI_disparity_screenin_rate['M'] = AI_white_screenin_rate['M']-AI_black_screenin_rate['M']\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,6), dpi=160)\n",
    "\n",
    "#x_ticks = range(10, 21)\n",
    "# plot lines\n",
    "plt.plot(list(AI_overall_screenin_rate.keys())[9:], list(AI_overall_screenin_rate.values())[9:], color = 'tab:green', label = \"AFST-only overall\", linestyle=\"-\", marker=\"x\")\n",
    "plt.plot(list(AI_black_screenin_rate.keys())[9:], list(AI_black_screenin_rate.values())[9:], color = 'tab:blue', label = \"AFST-only Black\", linestyle=\"--\", marker='o')\n",
    "plt.plot(list(AI_white_screenin_rate.keys())[9:], list(AI_white_screenin_rate.values())[9:], color = 'tab:orange', label = \"AFST-only white\", linestyle='-.', marker='s')\n",
    "\n",
    "plt.axhline(humanAI_overall_screenin_rate, color = 'tab:green', label = \"worker-AFST overall\", linestyle=\"-\")\n",
    "plt.axhline(humanAI_black_screenin_rate, color = 'tab:orange', label = \"worker-AFST white\", linestyle=\"--\")\n",
    "plt.axhline(humanAI_white_screenin_rate, color = 'tab:blue', label = \"worker-AFST Black\", linestyle=\"-.\")\n",
    "# plt.plot(x_ticks, AI_white_screenin_rate[9:], color = 'lightgrey', label = \"White Accuracy\", linestyle=\"-\", marker='.')\n",
    "\n",
    "# plt.plot(x_ticks, AI_disparity[9:], label = \"Disparity\", linestyle=\"-\", marker='.')\n",
    "plt.xlabel('AFST decision threshold (risk score) + M for mandatory screen-in', fontsize=16)\n",
    "plt.ylabel('Screen-in rate (\\%)', fontsize=16)\n",
    "#plt.xticks(x_ticks)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xticks(fontsize=14)\n",
    "\n",
    "legend = plt.legend(prop={'size': 14})\n",
    "for handle in legend.legendHandles:\n",
    "    handle.set_linewidth(2.0)\n",
    "    \n",
    "plt.savefig('../../figures/screenin_rate_per_threshold.png', dpi=600, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question: Can algorithm make different decisions between individuals in a referral?  \n",
    "# e.g. screen-in some / screen-out some in a referral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot disparities pre- & post-AFST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disparity_calc(data, decision_type, threshold, metric):\n",
    "    # decision_type options: 'workerAFST', 'AFST_threshold', 'AFST_mandatory'\n",
    "    # set up predictions & outcomes\n",
    "    if decision_type == 'AFST_threshold':\n",
    "        y_pred = data['MAX_score'] >= threshold\n",
    "    elif decision_type == 'workerAFST':\n",
    "        y_pred = data['CALL_SCRN_CODE']\n",
    "    elif decision_type == 'AFST_mandatory':\n",
    "        y_pred = data['MANDATORY_NULL']\n",
    "        \n",
    "    y_true = data['HighRiskOutcome']\n",
    "    sens_attrs = data['RACE']\n",
    "\n",
    "    # calculate disparity\n",
    "    mf = MetricFrame(metric=metric, y_true=y_true, y_pred=y_pred, sensitive_features=sens_attrs)\n",
    "    return abs(mf.by_group['white']-mf.by_group['Black'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pre_post_disparity(data_pre = pre_df, data_post = post_df, metric = accuracy_score, ylabel='Accuracy'):\n",
    "    \n",
    "    # calculate disparities\n",
    "    workerAFST_disparity_pre = disparity_calc(data=data_pre, decision_type='workerAFST', threshold=0, metric=metric)\n",
    "    workerAFST_disparity_post = disparity_calc(data=data_post, decision_type='workerAFST', threshold=0, metric=metric)\n",
    "    \n",
    "    AFSTonly_disparity_pre = {}\n",
    "    AFSTonly_disparity_post = {}\n",
    "    \n",
    "    for score in range(1,21):\n",
    "        AFSTonly_disparity_pre[score] = disparity_calc(data=data_pre, decision_type='AFST_threshold', threshold=score, metric=metric)\n",
    "        AFSTonly_disparity_post[score] = disparity_calc(data=data_post, decision_type='AFST_threshold', threshold=score, metric=metric)\n",
    "    \n",
    "    AFSTonly_disparity_pre['M'] = disparity_calc(data=data_pre, decision_type='AFST_mandatory', threshold=0, metric=metric)\n",
    "    AFSTonly_disparity_post['M'] = disparity_calc(data=data_post, decision_type='AFST_mandatory', threshold=0, metric=metric)\n",
    "    \n",
    "    # plot\n",
    "    plt.figure(figsize=(10,6), dpi=160)\n",
    "    plt.plot(list(AFSTonly_disparity_pre.keys())[9:], list(AFSTonly_disparity_pre.values())[9:], color = 'tab:orange', label = \"AFST-only (2015-2016)\", linestyle=\"--\", marker=\"x\")\n",
    "    plt.plot(list(AFSTonly_disparity_post.keys())[9:], list(AFSTonly_disparity_post.values())[9:], color = 'tab:blue', label = \"AFST-only (2016-2018)\", linestyle=\"-\", marker='o')\n",
    "\n",
    "    plt.axhline(workerAFST_disparity_pre, color = 'tab:green', label = \"worker-only (2015-2016)\", linestyle=\"--\")\n",
    "    plt.axhline(workerAFST_disparity_post, color = 'tab:red', label = \"worker-AFST (2016-2018)\", linestyle=\"-\")\n",
    "\n",
    "    plt.xlabel('AFST decision threshold (risk score) + M for mandatory screen-in', fontsize=16)\n",
    "    plt.ylabel(f'{ylabel} disparity (\\%)', fontsize=16)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.xticks(fontsize=12)\n",
    "\n",
    "    legend = plt.legend(prop={'size': 12})\n",
    "    for handle in legend.legendHandles:\n",
    "        handle.set_linewidth(2.0)\n",
    "        \n",
    "    fig_file_dict = {'True Positive Rate':'tpr',\n",
    "                    'Screen-in rate':'screen_in_rate',\n",
    "                    'Accuracy':'accuracy',\n",
    "                    'Screen-in rate':'screen_in_rate',\n",
    "                    'False Positive Rate':'fpr',\n",
    "                    'Precision':'precision'}\n",
    "\n",
    "    plt.savefig(f'../../figures/{fig_file_dict[ylabel]}_disparity.png', dpi=600, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pre_df['MAX_score'] >= 15\n",
    "y_true = pre_df['HighRiskOutcome']\n",
    "sens_attrs = pre_df['RACE']\n",
    "\n",
    "# calculate disparity\n",
    "mf = MetricFrame(metric=accuracy_score, y_true=y_true, y_pred=y_pred, sensitive_features=sens_attrs)\n",
    "mf.by_group['white']-mf.by_group['Black']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pre_df['CALL_SCRN_CODE']\n",
    "mf = MetricFrame(metric=accuracy_score, y_true=y_true, y_pred=y_pred, sensitive_features=sens_attrs)\n",
    "mf.by_group['white']-mf.by_group['Black']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pre_post_disparity(metric=selection_rate, ylabel='Screen-in rate')\n",
    "plot_pre_post_disparity(metric=accuracy_score, ylabel='Accuracy')\n",
    "plot_pre_post_disparity(metric=true_positive_rate, ylabel='True Positive Rate')\n",
    "plot_pre_post_disparity(metric=false_positive_rate, ylabel='False Positive Rate')\n",
    "plot_pre_post_disparity(metric=precision_score, ylabel='Precision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate human-AI accuracy\n",
    "# set up predictions & outcomes\n",
    "human_AI_decision = post_df['CALL_SCRN_CODE']\n",
    "y_true = post_df['HighRiskOutcome']\n",
    "y_pred = human_AI_decision\n",
    "sens_attrs = post_df['RACE']\n",
    "\n",
    "# calculate accuracy\n",
    "acc_worker = MetricFrame(metric=accuracy_score, y_true=y_true, y_pred=y_pred, sensitive_features = sens_attrs)\n",
    "\n",
    "# print\n",
    "print(\"Human-AI Decisions\")\n",
    "print(\"Overall Accuracy: \", acc_worker.overall)\n",
    "print(\"Group Accuracy: \", acc_worker.by_group[['white','Black']])\n",
    "humanAI_accuracy = [acc_worker.overall, acc_worker.by_group['white'], acc_worker.by_group['Black']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AI_overall_accuracy = {}\n",
    "AI_black_accuracy = {}\n",
    "AI_white_accuracy = {}\n",
    "AI_disparity_accuracy = {}\n",
    "y_true = post_df['HighRiskOutcome']\n",
    "sens_attrs = post_df['RACE']\n",
    "\n",
    "for i in range(1,21):\n",
    "    y_pred = post_df['MAX_score'] >= i\n",
    "    acc_ai = MetricFrame(metric = accuracy_score, y_true = y_true, y_pred = y_pred, sensitive_features = sens_attrs)\n",
    "    \n",
    "    AI_overall_accuracy[i] = acc_ai.overall\n",
    "    AI_black_accuracy[i] = acc_ai.by_group['Black']\n",
    "    AI_white_accuracy[i] = acc_ai.by_group['white']\n",
    "    AI_disparity_accuracy[i] = acc_ai.by_group['white'] - acc_ai.by_group['Black']\n",
    "    \n",
    "## calculate accuracy for mandatory screen-ins\n",
    "# set up predictions & outcomes\n",
    "AI_decision = post_df['MANDATORY_NULL']\n",
    "y_true = post_df['HighRiskOutcome']\n",
    "y_pred = AI_decision\n",
    "sens_attrs = post_df['RACE']\n",
    "\n",
    "# calculate accuracy\n",
    "acc_mand = MetricFrame(metric=accuracy_score, y_true=y_true, y_pred=y_pred, sensitive_features = sens_attrs)\n",
    "\n",
    "AI_overall_accuracy['M'] = acc_mand.overall\n",
    "AI_black_accuracy['M'] = acc_mand.by_group['Black']\n",
    "AI_white_accuracy['M'] = acc_mand.by_group['white']\n",
    "AI_disparity_accuracy['M'] = acc_mand.by_group['white'] - acc_ai.by_group['Black']\n",
    "\n",
    "AI_accuracy_mand = [acc_mand.overall, acc_mand.by_group['white'], acc_mand.by_group['Black']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6), dpi=160)\n",
    "\n",
    "#x_ticks = range(10, 21)\n",
    "# plot lines\n",
    "plt.plot(list(AI_overall_accuracy.keys())[9:], list(AI_overall_accuracy.values())[9:], color = 'tab:green', label = \"AFST-only overall\", linestyle=\"-\", marker=\"x\")\n",
    "plt.plot(list(AI_black_accuracy.keys())[9:], list(AI_black_accuracy.values())[9:], color = 'tab:blue', label = \"AFST-only Black\", linestyle=\"--\", marker='o')\n",
    "plt.plot(list(AI_white_accuracy.keys())[9:], list(AI_white_accuracy.values())[9:], color = 'tab:orange', label = \"AFST-only white\", linestyle='-.', marker='s')\n",
    "\n",
    "plt.axhline(humanAI_accuracy[0], color = 'tab:green', label = \"worker-AFST overall\", linestyle=\"-\")\n",
    "plt.axhline(humanAI_accuracy[1], color = 'tab:orange', label = \"worker-AFST white\", linestyle=\"--\")\n",
    "plt.axhline(humanAI_accuracy[2], color = 'tab:blue', label = \"worker-AFST Black\", linestyle=\"-.\")\n",
    "# plt.plot(x_ticks, AI_white_accuracy[9:], color = 'lightgrey', label = \"White Accuracy\", linestyle=\"-\", marker='.')\n",
    "\n",
    "# plt.plot(x_ticks, AI_disparity[9:], label = \"Disparity\", linestyle=\"-\", marker='.')\n",
    "plt.xlabel('AFST decision threshold (risk score) + M for mandatory screen-in', fontsize=16)\n",
    "plt.ylabel('Accuracy (\\%)', fontsize=16)\n",
    "#plt.xticks(x_ticks)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "\n",
    "legend = plt.legend(prop={'size': 14})\n",
    "for handle in legend.legendHandles:\n",
    "    handle.set_linewidth(2.0)\n",
    "    \n",
    "plt.savefig(f'../../figures/accuracy_per_threshold.png', dpi=600, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(round(AI_white_accuracy[15]*100,1))\n",
    "print(round(AI_overall_accuracy[15]*100,1))\n",
    "print(round(AI_black_accuracy[15]*100,1))\n",
    "print(round((AI_white_accuracy[15]-AI_black_accuracy[14])*100,1))\n",
    "\n",
    "print(round(humanAI_accuracy[1]*100,1))\n",
    "print(round(humanAI_accuracy[0]*100,1))\n",
    "print(round(humanAI_accuracy[2]*100,1))\n",
    "print(round((humanAI_accuracy[1]-humanAI_accuracy[2])*100,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_black = {'':-10, 'AFST-only\\n(2016-18)': 100*AI_black_accuracy[15], 'worker-AFST\\n(2016-18)': round(humanAI_accuracy[2]*100,2), ' ':-10}\n",
    "data_white = {'AFST-only\\n(2016-18)': 100*AI_white_accuracy[15], 'worker-AFST\\n(2016-18)': round(humanAI_accuracy[1]*100,2)}\n",
    "data_all = {'AFST-only\\n(2016-18)': 100*AI_overall_accuracy[15], 'worker-AFST\\n(2016-18)': round(humanAI_accuracy[0]*100,2)}\n",
    "\n",
    "legend_elements = [Line2D([0], [0], marker='o', color='w', label='Black children', markerfacecolor='tab:blue', markersize=7),\n",
    "                   Line2D([0], [0], marker='o', color='w', label='white children', markerfacecolor='tab:orange', markersize=7),\n",
    "                  Line2D([0], [0], marker='o', color='w', label='all children', markerfacecolor='tab:green', markersize=7)]\n",
    "\n",
    "line_afstonly = [[1, data_black['AFST-only\\n(2016-18)']],[1, data_white['AFST-only\\n(2016-18)']]]\n",
    "line_workerafst = [[2, data_black['worker-AFST\\n(2016-18)']],[2, data_white['worker-AFST\\n(2016-18)']]]\n",
    "\n",
    "plt.figure(figsize=(5, 4), dpi=160)\n",
    "plt.scatter(data_black.keys(), data_black.values(),zorder=2)\n",
    "plt.scatter(data_white.keys(), data_white.values(),zorder=2)\n",
    "plt.scatter(data_all.keys(), data_all.values(),zorder=2)\n",
    "\n",
    "plt.plot([line_afstonly[0][0], line_afstonly[1][0]], [line_afstonly[0][1], line_afstonly[1][1]], color='r', linestyle=':',zorder=1)\n",
    "plt.plot([line_workerafst[0][0], line_workerafst[1][0]], [line_workerafst[0][1], line_workerafst[1][1]], color='r', linestyle=':',zorder=1)\n",
    "\n",
    "plt.text(1.1,(data_black['AFST-only\\n(2016-18)']+data_white['AFST-only\\n(2016-18)'])/2-0.5, str(round(data_white['AFST-only\\n(2016-18)']-data_black['AFST-only\\n(2016-18)'],1))+'\\% \\ndisparity',color='r')\n",
    "plt.text(2.1,(data_black['worker-AFST\\n(2016-18)']+data_white['worker-AFST\\n(2016-18)'])/2-0.5, str(round(data_white['worker-AFST\\n(2016-18)']-data_black['worker-AFST\\n(2016-18)'],1))+'\\% \\ndisparity',color='r',zorder=3)\n",
    "plt.ylim(40,60)\n",
    "axes = plt.gca()\n",
    "axes.yaxis.grid()\n",
    "plt.title('Black-white accuracy disparities')\n",
    "plt.ylabel('Accuracy rate (\\%)',fontsize=14)\n",
    "plt.legend(handles=legend_elements)\n",
    "plt.savefig(f'../../figures/change_in_accuracy.png', dpi=600, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(round(AI_black_accuracy[15]*100,1))\n",
    "print(round(AI_white_accuracy[15]*100,1))\n",
    "print(round(AI_overall_accuracy[15]*100,1))\n",
    "\n",
    "print(round(humanAI_accuracy[2]*100,1))\n",
    "print(round(humanAI_accuracy[1]*100,1))\n",
    "print(round(humanAI_accuracy[0]*100,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-afst accuracy\n",
    "print(\"AFST-only:\",round(np.mean((pre_df['MAX_score'] >= i) == pre_df['HighRiskOutcome'])*100,1))\n",
    "print(\"worker-only:\",round(np.mean(pre_df['CALL_SCRN_CODE'] == pre_df['HighRiskOutcome'])*100,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Precision, FPR, & TPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FN = negative prediction, positive true\n",
    "# FP = positive prediction, negative true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate tpr for referrals from August 2016 to May 2018\n",
    "\n",
    "# set up predictions & outcomes\n",
    "human_AI_decision = post_df['CALL_SCRN_CODE']\n",
    "y_true = post_df['HighRiskOutcome']\n",
    "y_pred = human_AI_decision\n",
    "sens_attrs = post_df['RACE']\n",
    "\n",
    "# calculate tpr\n",
    "acc_worker = MetricFrame(metric=true_positive_rate, y_true=y_true, y_pred=y_pred, sensitive_features = sens_attrs)\n",
    "humanAI_tpr = [acc_worker.overall, acc_worker.by_group['white'], acc_worker.by_group['Black']]\n",
    "\n",
    "## AFST-only\n",
    "AI_overall_tpr = {}\n",
    "AI_black_tpr = {}\n",
    "AI_white_tpr = {}\n",
    "AI_disparity_tpr = {}\n",
    "y_true = post_df['HighRiskOutcome']\n",
    "sens_attrs = post_df['RACE']\n",
    "\n",
    "for i in range(1,21):\n",
    "    AI_decision = post_df['MAX_score'] >= i\n",
    "    y_pred = AI_decision\n",
    "    tpr_ai = MetricFrame(metric=true_positive_rate, y_true=y_true, y_pred=y_pred, sensitive_features = sens_attrs)\n",
    "    \n",
    "    AI_overall_tpr[i] = tpr_ai.overall\n",
    "    AI_black_tpr[i] = tpr_ai.by_group['Black']\n",
    "    AI_white_tpr[i] = tpr_ai.by_group['white']\n",
    "    AI_disparity_tpr[i] = tpr_ai.by_group['white'] - tpr_ai.by_group['Black']\n",
    "    \n",
    "## calculate tpr for mandatory screen-ins\n",
    "# set up predictions & outcomes\n",
    "AI_decision = post_df['MANDATORY_NULL']\n",
    "y_true = post_df['HighRiskOutcome']\n",
    "y_pred = AI_decision\n",
    "sens_attrs = post_df['RACE']\n",
    "\n",
    "# calculate tpr\n",
    "tpr_mand = MetricFrame(metric=true_positive_rate, y_true=y_true, y_pred=y_pred, sensitive_features = sens_attrs)\n",
    "\n",
    "AI_overall_tpr['M'] = tpr_mand.overall\n",
    "AI_black_tpr['M'] = tpr_mand.by_group['Black']\n",
    "AI_white_tpr['M'] = tpr_mand.by_group['white']\n",
    "AI_disparity_tpr['M'] = tpr_mand.by_group['white'] - tpr_ai.by_group['Black']\n",
    "\n",
    "AI_tpr_mand = [tpr_mand.overall, tpr_mand.by_group['white'], tpr_mand.by_group['Black']]\n",
    "\n",
    "## Plot\n",
    "plt.figure(figsize=(10,6), dpi=160)\n",
    "\n",
    "#x_ticks = range(10, 21)\n",
    "# plot lines\n",
    "plt.plot(list(AI_overall_tpr.keys())[9:], list(AI_overall_tpr.values())[9:], color = 'tab:green', label = \"AFST-only overall\", linestyle=\"-\", marker=\"x\")\n",
    "plt.plot(list(AI_white_tpr.keys())[9:], list(AI_white_tpr.values())[9:], color = 'tab:orange', label = \"AFST-only white\", linestyle='-.', marker='s')\n",
    "plt.plot(list(AI_black_tpr.keys())[9:], list(AI_black_tpr.values())[9:], color = 'tab:blue', label = \"AFST-only Black\", linestyle=\"--\", marker='o')\n",
    "\n",
    "plt.axhline(humanAI_tpr[0], color = 'tab:green', label = \"worker-AFST overall\", linestyle=\"-\")\n",
    "plt.axhline(humanAI_tpr[1], color = 'tab:orange', label = \"worker-AFST white\", linestyle=\"--\")\n",
    "plt.axhline(humanAI_tpr[2], color = 'tab:blue', label = \"worker-AFST Black\", linestyle=\"-.\")\n",
    "# plt.plot(x_ticks, AI_white_tpr[9:], color = 'lightgrey', label = \"White True Positive Rate\", linestyle=\"-\", marker='.')\n",
    "\n",
    "# plt.plot(x_ticks, AI_disparity_tpr[9:], label = \"Disparity\", linestyle=\"-\", marker='.')\n",
    "plt.xlabel('AFST decision threshold (risk score) + M for mandatory screen-in', fontsize=16)\n",
    "plt.ylabel('True Positive Rate (\\%)', fontsize=16)\n",
    "#plt.xticks(x_ticks)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "\n",
    "legend = plt.legend(prop={'size': 12})\n",
    "for handle in legend.legendHandles:\n",
    "    handle.set_linewidth(2.0)\n",
    "    \n",
    "plt.savefig(f'../../figures/tpr_per_threshold.png', dpi=600, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate fpr for referrals from August 2016 to May 2018\n",
    "\n",
    "# set up predictions & outcomes\n",
    "human_AI_decision = post_df['CALL_SCRN_CODE']\n",
    "y_true = post_df['HighRiskOutcome']\n",
    "y_pred = human_AI_decision\n",
    "sens_attrs = post_df['RACE']\n",
    "\n",
    "# calculate fpr\n",
    "acc_worker = MetricFrame(metric=false_positive_rate, y_true=y_true, y_pred=y_pred, sensitive_features = sens_attrs)\n",
    "humanAI_fpr = [acc_worker.overall, acc_worker.by_group['white'], acc_worker.by_group['Black']]\n",
    "\n",
    "## AFST-only\n",
    "AI_overall_fpr = {}\n",
    "AI_black_fpr = {}\n",
    "AI_white_fpr = {}\n",
    "AI_disparity_fpr = {}\n",
    "y_true = post_df['HighRiskOutcome']\n",
    "sens_attrs = post_df['RACE']\n",
    "\n",
    "for i in range(1,21):\n",
    "    AI_decision = post_df['MAX_score'] >= i\n",
    "    y_pred = AI_decision\n",
    "    fpr_ai = MetricFrame(metric=false_positive_rate, y_true=y_true, y_pred=y_pred, sensitive_features = sens_attrs)\n",
    "    \n",
    "    AI_overall_fpr[i] = fpr_ai.overall\n",
    "    AI_black_fpr[i] = fpr_ai.by_group['Black']\n",
    "    AI_white_fpr[i] = fpr_ai.by_group['white']\n",
    "    AI_disparity_fpr[i] = fpr_ai.by_group['white'] - fpr_ai.by_group['Black']\n",
    "    \n",
    "## calculate fpr for mandatory screen-ins\n",
    "# set up predictions & outcomes\n",
    "AI_decision = post_df['MANDATORY_NULL']\n",
    "y_true = post_df['HighRiskOutcome']\n",
    "y_pred = AI_decision\n",
    "sens_attrs = post_df['RACE']\n",
    "\n",
    "# calculate fpr\n",
    "fpr_mand = MetricFrame(metric=false_positive_rate, y_true=y_true, y_pred=y_pred, sensitive_features = sens_attrs)\n",
    "\n",
    "AI_overall_fpr['M'] = fpr_mand.overall\n",
    "AI_black_fpr['M'] = fpr_mand.by_group['Black']\n",
    "AI_white_fpr['M'] = fpr_mand.by_group['white']\n",
    "AI_disparity_fpr['M'] = fpr_mand.by_group['white'] - fpr_ai.by_group['Black']\n",
    "\n",
    "AI_fpr_mand = [fpr_mand.overall, fpr_mand.by_group['white'], fpr_mand.by_group['Black']]\n",
    "\n",
    "## Plot\n",
    "plt.figure(figsize=(10,6), dpi=160)\n",
    "\n",
    "#x_ticks = range(10, 21)\n",
    "# plot lines\n",
    "plt.plot(list(AI_overall_fpr.keys())[9:], list(AI_overall_fpr.values())[9:], color = 'tab:green', label = \"AFST-only overall\", linestyle=\"-\", marker=\"x\")\n",
    "plt.plot(list(AI_white_fpr.keys())[9:], list(AI_white_fpr.values())[9:], color = 'tab:orange', label = \"AFST-only white\", linestyle='-.', marker='s')\n",
    "plt.plot(list(AI_black_fpr.keys())[9:], list(AI_black_fpr.values())[9:], color = 'tab:blue', label = \"AFST-only Black\", linestyle=\"--\", marker='o')\n",
    "\n",
    "plt.axhline(humanAI_fpr[0], color = 'tab:green', label = \"worker-AFST overall\", linestyle=\"-\")\n",
    "plt.axhline(humanAI_fpr[1], color = 'tab:orange', label = \"worker-AFST white\", linestyle=\"--\")\n",
    "plt.axhline(humanAI_fpr[2], color = 'tab:blue', label = \"worker-AFST Black\", linestyle=\"-.\")\n",
    "# plt.plot(x_ticks, AI_white_fpr[9:], color = 'lightgrey', label = \"White False Positive Rate\", linestyle=\"-\", marker='.')\n",
    "\n",
    "# plt.plot(x_ticks, AI_disparity_fpr[9:], label = \"Disparity\", linestyle=\"-\", marker='.')\n",
    "plt.xlabel('AFST decision threshold (risk score) + M for mandatory screen-in', fontsize=16)\n",
    "plt.ylabel('False Positive Rate (\\%)', fontsize=16)\n",
    "#plt.xticks(x_ticks)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "\n",
    "legend = plt.legend(prop={'size': 14})\n",
    "for handle in legend.legendHandles:\n",
    "    handle.set_linewidth(2.0)\n",
    "    \n",
    "plt.savefig(f'../../figures/fpr_per_threshold.png', dpi=600, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate precision for referrals from August 2016 to May 2018\n",
    "\n",
    "# arial font\n",
    "plt.rcParams[\"font.family\"] = 'sans-serif'\n",
    "\n",
    "# set up predictions & outcomes\n",
    "human_AI_decision = post_df['CALL_SCRN_CODE']\n",
    "y_true = post_df['HighRiskOutcome']\n",
    "y_pred = human_AI_decision\n",
    "sens_attrs = post_df['RACE']\n",
    "\n",
    "# calculate precision\n",
    "acc_worker = MetricFrame(metric=precision_score, y_true=y_true, y_pred=y_pred, sensitive_features = sens_attrs)\n",
    "humanAI_precision = [acc_worker.overall, acc_worker.by_group['white'], acc_worker.by_group['Black']]\n",
    "\n",
    "## AFST-only\n",
    "AI_overall_precision = {}\n",
    "AI_black_precision = {}\n",
    "AI_white_precision = {}\n",
    "AI_disparity_precision = {}\n",
    "y_true = post_df['HighRiskOutcome']\n",
    "sens_attrs = post_df['RACE']\n",
    "\n",
    "for i in range(1,21):\n",
    "    AI_decision = post_df['MAX_score'] >= i\n",
    "    y_pred = AI_decision\n",
    "    precision_ai = MetricFrame(metric=precision_score, y_true=y_true, y_pred=y_pred, sensitive_features = sens_attrs)\n",
    "    \n",
    "    AI_overall_precision[i] = precision_ai.overall\n",
    "    AI_black_precision[i] = precision_ai.by_group['Black']\n",
    "    AI_white_precision[i] = precision_ai.by_group['white']\n",
    "    AI_disparity_precision[i] = precision_ai.by_group['white'] - precision_ai.by_group['Black']\n",
    "    \n",
    "## calculate precision for mandatory screen-ins\n",
    "# set up predictions & outcomes\n",
    "AI_decision = post_df['MANDATORY_NULL']\n",
    "y_true = post_df['HighRiskOutcome']\n",
    "y_pred = AI_decision\n",
    "sens_attrs = post_df['RACE']\n",
    "\n",
    "# calculate precision\n",
    "precision_mand = MetricFrame(metric=precision_score, y_true=y_true, y_pred=y_pred, sensitive_features = sens_attrs)\n",
    "\n",
    "AI_overall_precision['M'] = precision_mand.overall\n",
    "AI_black_precision['M'] = precision_mand.by_group['Black']\n",
    "AI_white_precision['M'] = precision_mand.by_group['white']\n",
    "AI_disparity_precision['M'] = precision_mand.by_group['white'] - precision_ai.by_group['Black']\n",
    "\n",
    "AI_precision_mand = [precision_mand.overall, precision_mand.by_group['white'], precision_mand.by_group['Black']]\n",
    "\n",
    "## Plot\n",
    "plt.figure(figsize=(10,6), dpi=160)\n",
    "\n",
    "#x_ticks = range(10, 21)\n",
    "# plot lines\n",
    "plt.plot(list(AI_overall_precision.keys())[9:], list(AI_overall_precision.values())[9:], color = 'tab:green', label = \"AFST-only overall\", linestyle=\"-\", marker=\"x\")\n",
    "plt.plot(list(AI_white_precision.keys())[9:], list(AI_white_precision.values())[9:], color = 'tab:orange', label = \"AFST-only white\", linestyle='-.', marker='s')\n",
    "plt.plot(list(AI_black_precision.keys())[9:], list(AI_black_precision.values())[9:], color = 'tab:blue', label = \"AFST-only Black\", linestyle=\"--\", marker='o')\n",
    "\n",
    "plt.axhline(humanAI_precision[0], color = 'tab:green', label = \"worker-AFST overall\", linestyle=\"-\")\n",
    "plt.axhline(humanAI_precision[1], color = 'tab:orange', label = \"worker-AFST white\", linestyle=\"--\")\n",
    "plt.axhline(humanAI_precision[2], color = 'tab:blue', label = \"worker-AFST Black\", linestyle=\"-.\")\n",
    "# plt.plot(x_ticks, AI_white_precision[9:], color = 'lightgrey', label = \"White False Positive Rate\", linestyle=\"-\", marker='.')\n",
    "\n",
    "# plt.plot(x_ticks, AI_disparity_precision[9:], label = \"Disparity\", linestyle=\"-\", marker='.')\n",
    "plt.xlabel('AFST decision threshold (risk score) + M for mandatory screen-in', fontsize=16)\n",
    "plt.ylabel('Precision (\\%)', fontsize=16)\n",
    "#plt.xticks(x_ticks)\n",
    "plt.yticks(fontsize=12,fontfamily='sans-serif')\n",
    "plt.xticks(fontsize=12)\n",
    "\n",
    "legend = plt.legend(prop={'size': 14})\n",
    "for handle in legend.legendHandles:\n",
    "    handle.set_linewidth(2.0)\n",
    "    \n",
    "plt.savefig(f'../../figures/precision_per_threshold.png', dpi=600, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare disparity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "AI_disparity_screenin_rate[1]-AI_disparity_screenin_rate[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Non)compliance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_gb = post_df.groupby(['MAX_level','CALL_SCRN_CODE']).size()\n",
    "\n",
    "# % of screened-in High risk referrals\n",
    "print(comp_gb['High_risk',0])\n",
    "print(np.count_nonzero(post_df.MAX_level=='High_risk'))\n",
    "print(round(comp_gb['High_risk',0]/np.count_nonzero(post_df.MAX_level=='High_risk')*100,1))\n",
    "print()\n",
    "\n",
    "# Medium risk referrals\n",
    "print(comp_gb['Medium_risk',1])\n",
    "print(np.count_nonzero(post_df.MAX_level=='Medium_risk'))\n",
    "print(round((comp_gb['Medium_risk',1])/np.count_nonzero(post_df.MAX_level=='Medium_risk')*100,1))\n",
    "print()\n",
    "\n",
    "\n",
    "print(comp_gb['Low_risk',1])\n",
    "print(np.count_nonzero(post_df.MAX_level=='Low_risk'))\n",
    "print(round((comp_gb['Low_risk',1])/np.count_nonzero(post_df.MAX_level=='Low_risk')*100,1))\n",
    "print()\n",
    "\n",
    "print(comp_gb['Low_risk',1]+comp_gb['Medium_risk',1])\n",
    "print(np.count_nonzero(post_df.MAX_level=='Low_risk')+np.count_nonzero(post_df.MAX_level=='Medium_risk'))\n",
    "print(round((comp_gb['Low_risk',1]+comp_gb['Medium_risk',1])/(np.count_nonzero(post_df.MAX_level=='Low_risk')+np.count_nonzero(post_df.MAX_level=='Medium_risk'))*100,1))\n",
    "print()\n",
    "\n",
    "mand_gb = post_df.groupby(['MANDATORY_NULL','CALL_SCRN_CODE']).size()\n",
    "print(mand_gb[1,0])\n",
    "print(np.count_nonzero(post_df.MANDATORY_NULL==1))\n",
    "print(round(mand_gb[1,0]/np.count_nonzero(post_df.MANDATORY_NULL==1)*100,1))\n",
    "print()\n",
    "\n",
    "\n",
    "print((comp_gb['High_risk',0]+comp_gb['Medium_risk',0]+comp_gb['Low_risk',0]))\n",
    "print(len(post_df))\n",
    "print()\n",
    "\n",
    "print(round((comp_gb['High_risk',0]+comp_gb['Medium_risk',1]+comp_gb['Low_risk',1])/len(post_df)*100,1))\n",
    "print(round((comp_gb['High_risk',0]+comp_gb['Low_risk',1])/(len(post_df)-np.count_nonzero(post_df.MAX_level == 'Medium_risk'))*100,1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_gb = post_df.groupby(['MAX_level','CALL_SCRN_CODE','RACE']).size()\n",
    "\n",
    "comp_screenin_black = comp_gb['High_risk',1,'Black']\n",
    "noncomp_screenin_black = comp_gb['High_risk',0,'Black']\n",
    "comp_screenout_black = sum(comp_gb[:,1,'Black']) - comp_gb['High_risk',1,'Black']\n",
    "noncomp_screenout_black = sum(comp_gb[:,0,'Black']) - comp_gb['High_risk',0,'Black']\n",
    "total_rfrl_black = sum(comp_gb[:,:,'Black'])\n",
    "\n",
    "comp_screenin_white = comp_gb['High_risk',1,'white']\n",
    "noncomp_screenin_white = comp_gb['High_risk',0,'white']\n",
    "comp_screenout_white = sum(comp_gb[:,1,'white']) - comp_gb['High_risk',1,'white']\n",
    "noncomp_screenout_white = sum(comp_gb[:,0,'white']) - comp_gb['High_risk',0,'white']\n",
    "total_rfrl_white = sum(comp_gb[:,:,'white'])\n",
    "\n",
    "print(comp_screenin_black/total_rfrl_black)\n",
    "print(noncomp_screenin_black/total_rfrl_black)\n",
    "print(comp_screenout_black/total_rfrl_black)\n",
    "print(noncomp_screenout_black/total_rfrl_black)\n",
    "\n",
    "print(comp_screenin_white/total_rfrl_white)\n",
    "print(noncomp_screenin_white/total_rfrl_white)\n",
    "print(comp_screenout_white/total_rfrl_white)\n",
    "print(noncomp_screenout_white/total_rfrl_white)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1 = [comp_screenin_black/total_rfrl_black,\n",
    "         noncomp_screenin_black/total_rfrl_black,\n",
    "         comp_screenout_black/total_rfrl_black,\n",
    "         noncomp_screenout_black/total_rfrl_black]\n",
    "\n",
    "temp2 = [comp_screenin_white/total_rfrl_white,\n",
    "         noncomp_screenin_white/total_rfrl_white,\n",
    "         comp_screenout_white/total_rfrl_white,\n",
    "         noncomp_screenout_white/total_rfrl_white]\n",
    "\n",
    "labels = [\"AFST-only \" + r\"\\textbf{\" + 'screen in' + \"}\" + \"\\n worker-AFST \" + r\"\\textbf{\" + 'screen in' + \"}\",\n",
    "          \"AFST-only \" + r\"\\textbf{\" + 'screen in' + \"}\" + \"\\n worker-AFST \" + r\"\\textbf{\" + 'screen out' + \"}\",\n",
    "          \"AFST-only \" + r\"\\textbf{\" + 'screen out' + \"}\" + \"\\n worker-AFST \" + r\"\\textbf{\" + 'screen in' + \"}\",\n",
    "          \"AFST-only \" + r\"\\textbf{\" + 'screen out' + \"}\" + \"\\n worker-AFST \" + r\"\\textbf{\" + 'screen out' + \"}\"]\n",
    "\n",
    "N = len(labels)\n",
    "ind = np.arange(N)  # the x locations for the groups\n",
    "width = 0.35       # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4), dpi=160)\n",
    "\n",
    "rects1 = ax.bar(ind, temp1, width, color='tab:blue')\n",
    "rects2 = ax.bar(ind + width, temp2, width, color='tab:orange', edgecolor='grey', hatch=\"/\")\n",
    "\n",
    "# add some text for labels, title and axes ticks\n",
    "ax.set_ylabel('Percent of total referrals by race',fontsize=14)\n",
    "ax.set_xlabel('AFST-only vs. worker-AFST decision',fontsize=14,labelpad=15)\n",
    "ax.set_title('AFST-only / worker-AFST decisions as percents of total referred children by race',fontsize=14)\n",
    "ax.set_xticks(ind + width / 2)\n",
    "ax.set_xticklabels(labels)\n",
    "\n",
    "ax.legend((rects1[0], rects2[0]), ('Black children', 'white children'), fontsize=14)\n",
    "\n",
    "\n",
    "def rect_label(rects):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.text(rect.get_x() + rect.get_width()/2., .5*height,\n",
    "                str(round(height*100,1))+'\\%',\n",
    "                ha='center', va='bottom',fontsize=16)\n",
    "\n",
    "rect_label(rects1)\n",
    "rect_label(rects2)\n",
    "\n",
    "plt.savefig('../../figures/human-ai_compliance_breakdown.png', dpi=600, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total % of noncompliance between workers and AFST\n",
    "np.mean(post_df['CALL_SCRN_CODE'] == (post_df['MAX_score'] >= 15))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(post_df)\n",
    "post_df.RACE == 'Black'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_all = len(post_df)\n",
    "all_mand = len(post_df[post_df.MANDATORY_NULL==1])\n",
    "all_high = len(post_df[post_df.MAX_level=='High_risk'])\n",
    "all_med = len(post_df[post_df.MAX_level=='Medium_risk'])\n",
    "all_low = len(post_df[post_df.MAX_level=='Low_risk'])\n",
    "\n",
    "Black_all = len(post_df[post_df.RACE=='Black'])\n",
    "Black_mand = len(post_df[(post_df.MANDATORY_NULL==1)&(post_df.RACE=='Black')])\n",
    "Black_high = len(post_df[(post_df.MAX_level=='High_risk')&(post_df.RACE=='Black')])\n",
    "Black_med = len(post_df[(post_df.MAX_level=='Medium_risk')&(post_df.RACE=='Black')])\n",
    "Black_low = len(post_df[(post_df.MAX_level=='Low_risk')&(post_df.RACE=='Black')])\n",
    "\n",
    "white_all = len(post_df[post_df.RACE=='white'])\n",
    "white_mand = len(post_df[(post_df.MANDATORY_NULL==1)&(post_df.RACE=='white')])\n",
    "white_high = len(post_df[(post_df.MAX_level=='High_risk')&(post_df.RACE=='white')])\n",
    "white_med = len(post_df[(post_df.MAX_level=='Medium_risk')&(post_df.RACE=='white')])\n",
    "white_low = len(post_df[(post_df.MAX_level=='Low_risk')&(post_df.RACE=='white')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pct(num,denom):\n",
    "    return round(num/denom*100,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# race_risklevel\n",
    "print(f\"  &  All referral-children &  Black referral-children & white referral-children \\\\\\\\ \\hline\\nAll risk levels  & {all_all}   & {Black_all} ({pct(Black_all,all_all)}\\%) & {white_all} ({pct(white_all,all_all)}\\%) \\\\\\\\ \\hline\\nMandatory screen-in & {all_mand} ({pct(all_mand,all_all)}\\%) & {Black_mand} ({pct(Black_mand,Black_all)}\\%) & {white_mand} ({pct(white_mand,white_all)}\\%) \\\\\\\\ \\hline\\nHigh risk & {all_high} ({pct(all_high,all_all)}\\%) & {Black_high} ({pct(Black_high,Black_all)}\\%) & {white_high} ({pct(white_high,white_all)}\\%) \\\\\\\\ \\hline\\nMedium risk & {all_med} ({pct(all_med,all_all)}\\%) & {Black_med} ({pct(Black_med,Black_all)}\\%) & {white_med} ({pct(white_med,white_all)}\\%) \\\\\\\\ \\hline\\nLow risk & {all_low} ({pct(all_low,all_all)}\\%) & {Black_low} ({pct(Black_low,Black_all)}\\%) & {white_low} ({pct(white_low,white_all)}\\%) \\\\\\\\ \\hline\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare accuracy for each score / mandatory screen-ins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_df_mand = post_df[post_df['MANDATORY_NULL']==1]\n",
    "post_df_mand_screenin = post_df_mand[post_df_mand['CALL_SCRN_CODE']==1]\n",
    "np.count_nonzero(post_df_mand_screenin['HighRiskOutcome'])/len(post_df_mand_screenin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(post_df_mand['HighRiskOutcome'])/len(post_df_mand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by mandatory screen-in and max AFST scores\n",
    "test = post_df.groupby(['MANDATORY_NULL','CALL_SCRN_CODE','HighRiskOutcome']).size()\n",
    "test1 = post_df.groupby(['MAX_score','CALL_SCRN_CODE','HighRiskOutcome']).size()\n",
    "\n",
    "workerAFST_hro_binned = {}\n",
    "workerAFST_accuracy_binned = {}\n",
    "AFSTonly_accuracy_binned = {}\n",
    "AFSTonly_hro_binned = {}\n",
    "\n",
    "workerAFST_hro_binned_denom = {}\n",
    "workerAFST_accuracy_binned_denom = {}\n",
    "AFSTonly_accuracy_binned_denom = {}\n",
    "AFSTonly_hro_binned_denom = {}\n",
    "\n",
    "# measure scores 1-9 by screen out accuracy\n",
    "for score in range(1,10):\n",
    "    post_df_score = post_df[post_df['MAX_score']==score]\n",
    "    y_pred = post_df_score['CALL_SCRN_CODE']\n",
    "    y_true = 1 - post_df_score['HighRiskOutcome']\n",
    "    sens_attrs = post_df_score['RACE']\n",
    "    \n",
    "    workerAFST_accuracy_binned[score] = round(MetricFrame(metric=accuracy_score, y_true=y_true, y_pred=y_pred, sensitive_features=sens_attrs).overall*100,1)\n",
    "    workerAFST_accuracy_binned_denom[score] = len(sens_attrs)\n",
    "    \n",
    "    AFSTonly_accuracy_binned[score] = round(sum(test1[score][:,0])/sum(test1[score])*100,1)\n",
    "    AFSTonly_accuracy_binned_denom[score] = sum(test1[score])\n",
    "    \n",
    "    workerAFST_hro_binned[score] = round(test1[score][0,1]/sum(test1[score][1])*100,1)\n",
    "    workerAFST_hro_binned_denom[score] = sum(test1[score][0])\n",
    "    \n",
    "    \n",
    "    AFSTonly_hro_binned[score] = round(sum(test1[score][:,1])/sum(test1[score])*100,1)\n",
    "    AFSTonly_hro_binned_denom[score] = sum(test1[score])\n",
    "    \n",
    "# measure scores 15-20 by screen in accuracy\n",
    "for score in range(15,21):\n",
    "    post_df_score = post_df[post_df['MAX_score']==score]\n",
    "    y_pred = 1 - post_df_score['CALL_SCRN_CODE']\n",
    "    y_true = post_df_score['HighRiskOutcome']\n",
    "    sens_attrs = post_df_score['RACE']\n",
    "    \n",
    "    workerAFST_accuracy_binned[score] = round(MetricFrame(metric=accuracy_score, y_true=y_true, y_pred=y_pred, sensitive_features=sens_attrs).overall*100,1)\n",
    "    workerAFST_accuracy_binned_denom[score] = len(sens_attrs)\n",
    "    \n",
    "    AFSTonly_accuracy_binned[score] = round(sum(test1[score][:,1])/sum(test1[score])*100,1)\n",
    "    AFSTonly_accuracy_binned_denom[score] = sum(test1[score])\n",
    "    \n",
    "    workerAFST_hro_binned[score] = round(test1[score][0,1]/sum(test1[score][0])*100,1)\n",
    "    workerAFST_hro_binned_denom[score] = sum(test1[score][0])\n",
    "    \n",
    "    AFSTonly_hro_binned[score] = AFSTonly_accuracy_binned[score]\n",
    "    AFSTonly_hro_binned_denom[score] = AFSTonly_accuracy_binned_denom[score]\n",
    "    \n",
    "# add mandatory screen-in cases\n",
    "post_df_mand = post_df[post_df['MANDATORY_NULL']==1]\n",
    "y_pred = 1 - post_df_mand['CALL_SCRN_CODE']\n",
    "y_true = post_df_mand['HighRiskOutcome']\n",
    "sens_attrs = post_df_mand['RACE']\n",
    "\n",
    "workerAFST_accuracy_binned['M'] = round(MetricFrame(metric=accuracy_score, y_true=y_true, y_pred=y_pred, sensitive_features=sens_attrs).overall*100,1)\n",
    "workerAFST_accuracy_binned_denom['M'] = len(sens_attrs)\n",
    "\n",
    "AFSTonly_accuracy_binned['M'] = round(sum(test[1][:,1])/sum(test[1])*100,1)\n",
    "AFSTonly_accuracy_binned_denom['M'] = sum(test[1])\n",
    "\n",
    "workerAFST_hro_binned['M'] = round(test[1][0][1]/sum(test[1][0])*100,1)\n",
    "workerAFST_hro_binned_denom['M'] = sum(test[1][0])\n",
    "\n",
    "AFSTonly_hro_binned['M'] = AFSTonly_accuracy_binned['M']\n",
    "AFSTonly_hro_binned_denom['M'] = AFSTonly_accuracy_binned_denom['M']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workerAFST_accuracy_binned.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AFSTonly_accuracy_binned.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workerAFST_accuracy_binned_denom.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AFSTonly_hro_binned.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AFSTonly_hro_binned_denom.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workerAFST_hro_binned.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workerAFST_hro_binned_denom.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By race & risk level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(post_df))\n",
    "print(np.count_nonzero(post_df['MANDATORY_NULL']))\n",
    "print(np.count_nonzero(post_df['MAX_score']>=15))\n",
    "print(np.count_nonzero(post_df['MAX_level']=='Medium_risk'))\n",
    "print(np.count_nonzero(post_df['MAX_level']=='Low_risk'))\n",
    "\n",
    "print(round(np.count_nonzero(post_df['MANDATORY_NULL'])/len(post_df)*100,1))\n",
    "print(round(np.count_nonzero(post_df['MAX_score']>=15)/len(post_df)*100,1))\n",
    "print(round(np.count_nonzero(post_df['MAX_level']=='Medium_risk')/len(post_df)*100,1))\n",
    "print(round(np.count_nonzero(post_df['MAX_level']=='Low_risk')/len(post_df)*100,1))\n",
    "\n",
    "print()\n",
    "print(np.count_nonzero(post_df['RACE']=='Black'))\n",
    "print(np.count_nonzero(post_df['RACE']=='white'))\n",
    "\n",
    "print(round(np.count_nonzero(post_df['RACE']=='Black')/len(post_df)*100,1))\n",
    "print(round(np.count_nonzero(post_df['RACE']=='white')/len(post_df)*100,1))\n",
    "\n",
    "print()\n",
    "print(np.count_nonzero(post_df['MANDATORY_NULL'] & (post_df['RACE']=='Black')))\n",
    "print(np.count_nonzero((post_df['MAX_score']>=15) & (post_df['RACE']=='Black')))\n",
    "print(np.count_nonzero((post_df['MAX_level']=='Medium_risk') & (post_df['RACE']=='Black')))\n",
    "print(np.count_nonzero((post_df['MAX_level']=='Low_risk') & (post_df['RACE']=='Black')))\n",
    "\n",
    "print(round(np.count_nonzero(post_df['MANDATORY_NULL'] & (post_df['RACE']=='Black'))/np.count_nonzero(post_df['RACE']=='Black')*100,1))\n",
    "print(round(np.count_nonzero((post_df['MAX_score']>=15) & (post_df['RACE']=='Black'))/np.count_nonzero(post_df['RACE']=='Black')*100,1))\n",
    "print(round(np.count_nonzero((post_df['MAX_level']=='Medium_risk') & (post_df['RACE']=='Black'))/np.count_nonzero(post_df['RACE']=='Black')*100,1))\n",
    "print(round(np.count_nonzero((post_df['MAX_level']=='Low_risk') & (post_df['RACE']=='Black'))/np.count_nonzero(post_df['RACE']=='Black')*100,1))\n",
    "\n",
    "print()\n",
    "print(np.count_nonzero(post_df['MANDATORY_NULL'] & (post_df['RACE']=='white')))\n",
    "print(np.count_nonzero((post_df['MAX_score']>=15) & (post_df['RACE']=='white')))\n",
    "print(np.count_nonzero((post_df['MAX_level']=='Medium_risk') & (post_df['RACE']=='white')))\n",
    "print(np.count_nonzero((post_df['MAX_level']=='Low_risk') & (post_df['RACE']=='white')))\n",
    "\n",
    "print(round(np.count_nonzero(post_df['MANDATORY_NULL'] & (post_df['RACE']=='white'))/np.count_nonzero(post_df['RACE']=='white')*100,1))\n",
    "print(round(np.count_nonzero((post_df['MAX_score']>=15) & (post_df['RACE']=='white'))/np.count_nonzero(post_df['RACE']=='white')*100,1))\n",
    "print(round(np.count_nonzero((post_df['MAX_level']=='Medium_risk') & (post_df['RACE']=='white'))/np.count_nonzero(post_df['RACE']=='white')*100,1))\n",
    "print(round(np.count_nonzero((post_df['MAX_level']=='Low_risk') & (post_df['RACE']=='white'))/np.count_nonzero(post_df['RACE']=='white')*100,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(post_df.loc[post_df['MANDATORY_NULL']==1]['REFER_ID'].nunique()/post_df['REFER_ID'].nunique())\n",
    "print(len(post_df.loc[post_df['MANDATORY_NULL']==1])/len(post_df))\n",
    "print(len(post_df.loc[(post_df['MANDATORY_NULL']==1)&(post_df['CALL_SCRN_CODE']==1)])/len(post_df.loc[post_df['MANDATORY_NULL']==1]))\n",
    "          "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
